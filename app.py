import streamlit as st
import google.generativeai as genai
import edge_tts
import asyncio
from streamlit_mic_recorder import mic_recorder
from PIL import Image
import re
import io

# --- 1. CONFIGURATION ---
st.set_page_config(page_title="JARVIS", page_icon="ü§ñ", layout="centered")

# Custom JARVIS Styling
st.markdown("""
    <style>
        .stApp {background-color: #000000; color: #00ffcc;}
        .stButton>button {border-radius: 20px; background-color: #003333; color: #00ffcc; border: 1px solid #00ffcc;}
        /* Styling the search bar area */
        .stChatInputContainer {padding-bottom: 20px;}
    </style>
""", unsafe_allow_html=True)

# API Setup
if "GOOGLE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
else:
    st.error("API Key Missing")
    st.stop()

# --- 2. LOGIC FUNCTIONS ---
def clean_text(text):
    """Removes Markdown symbols for voice synthesis."""
    return re.sub(r'[*#`]', '', text).strip()

async def speak(text):
    OUTPUT_FILE = "jarvis_reply.mp3"
    spoken_text = clean_text(text)
    communicate = edge_tts.Communicate(spoken_text, "en-GB-RyanNeural")
    await communicate.save(OUTPUT_FILE)
    return OUTPUT_FILE

# --- 3. THE INTERFACE ---
st.markdown("<h1 style='text-align: center; color: #00ffcc; font-family: Courier New;'>J.A.R.V.I.S.</h1>", unsafe_allow_html=True)

# Vision Toggle
use_vision = st.checkbox("üëÅÔ∏è Enable Vision System")
camera_image = st.camera_input("Visual Feed") if use_vision else None

# Voice Component
c1, c2, c3 = st.columns([1, 2, 1])
with c2:
    audio_input = mic_recorder(start_prompt="üî¥ VOICE COMMAND", stop_prompt="‚èπÔ∏è STOP", key="recorder", just_once=True)

# Search Bar (Typing Input)
typed_input = st.chat_input("Type your command, Sir...")

# --- 4. EXECUTION ---
# Trigger if either Voice or Typing is used
user_prompt = None
audio_bytes = None

if audio_input:
    audio_bytes = audio_input['bytes']
    user_prompt = "Process the attached audio command."
elif typed_input:
    user_prompt = typed_input

if user_prompt:
    status = st.empty()
    status.info("‚ö° JARVIS is thinking...")

    try:
        # Use Gemini 2.0 Flash for native image generation support
        model = genai.GenerativeModel("gemini-2.0-flash")
        
        inputs = []
        if camera_image:
            inputs.append(Image.open(camera_image))
        if audio_bytes:
            inputs.append({"mime_type": "audio/wav", "data": audio_bytes})
        
        inputs.append(f"You are JARVIS. {user_prompt}. If asked to create/generate an image, do so. Otherwise, reply concisely.")

        # Configuration for Image Generation
        # Note: 'response_modalities' allows the model to return images directly
        response = model.generate_content(
            inputs,
            generation_config={"response_modalities": ["TEXT", "IMAGE"]}
        )

        # Handle Multiple Output Parts (Text and/or Images)
        for part in response.candidates[0].content.parts:
            if hasattr(part, 'text') and part.text:
                st.success(f"ü§ñ {part.text}")
                # Voice output for the text part
                audio_file = asyncio.run(speak(part.text))
                st.audio(audio_file, format='audio/mp3', autoplay=True)
            
            if hasattr(part, 'inline_data') and part.inline_data:
                # This handles the generated image
                img_data = part.inline_data.data
                st.image(img_data, caption="Generated by JARVIS", use_container_width=True)

    except Exception as e:
        status.error(f"SYSTEM ERROR: {e}")
